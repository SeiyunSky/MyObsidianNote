### Tensor
Tensor是PyTorch里对多维数组的表示。可以用它来表示：
**标量（0维）**：单个数，比如 `torch.tensor(3.14)`
**向量（1维）**：一列数，比如 `torch.tensor([1,2,3])`
**矩阵（2维）**：行列数据，比如 `torch.tensor([[1,2],[3,4]])`
**高维张量（3维及以上）**：高维数据，比如`torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])`

PyTorch里的数据类型，主要为：
**整数型** torch.uint8、torch.int32、torch.int64。其中torch.int64为默认的整数类型。
**浮点型** torch.float16、torch.bfloat16、 torch.float32、torch.float64，其中torch.float32为默认的浮点数据类型。
**布尔型** torch.bool
```python
x = torch.tensor([1, 2, 3, 4, 5])
#这里的mask会对x内每个元素做一次是否大于2的操作
mask = x > 2  # 生成一个布尔掩码

#利用布尔掩码选出满足需求的值
filtered_x = x[mask]
```

**利用指定值或随机值来创建tensor**
```python
shape = (2,3)
rand_tensor = torch.rand(shape) # 生成一个从[0,1]均匀抽样的tensor。
randn_tensor = torch.randn(shape) # 生成一个从标准正态分布抽样的tensor。
ones_tensor = torch.ones(shape) #生成一个值全为1的tensor。
zeros_tensor = torch.zeros(shape) # 生成一个值全为0的tensor。
twos_tensor = torch.full(shape, 2) #  生成一个值全为2的tensor。
```

**tensor的属性**
tensor.shape
tensor.dtype
tensor.device
tensor.requires_grad

**tensor常用的操作**
**形状变换**
```python
x = torch.randn(4,4) #  生成一个形状为4x4的随机矩阵。
x = x.reshape(2,8) # 通过reshape操作，可以将4x4的矩阵改变为2x8的矩阵。
```
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
x_reshape = x.reshape(3,2)
x_transpose = x.permute(1,0) #交换第0个和第1个维度。对于二维矩阵就是行列互换，进行转置。
print("reshape:",x_reshape)
print("permute:",x_transpose)
```
变化形态和转置的区别
```output
reshape: tensor([[1, 2],
        [3, 4],
        [5, 6]])
permute: tensor([[1, 4],
        [2, 5],
        [3, 6]])
```
对于二维tensor，可以调用`tensor.t()`方法进行转置操作
```python
a @ b.t()  # 矩阵乘法
```
**拓展维度**
```python
x = torch.tensor([[1,2,3],[4,5,6]])
#扩展第0维:将原始 2×3 矩阵变成一个 ​1×2×3 的张量
x_0 = x.unsqueeze(0)
print(x_0.shape,x_0)
#扩展第1维:​变成一个 ​2×1×3 的张量
x_1 = x.unsqueeze(1)
print(x_1.shape,x_1)
#扩展第2维:在第 2 维（即最内层）插入一个长度为 1 的新维度
x_2 = x.unsqueeze(2)
print(x_2.shape,x_2)
```
```output
torch.Size([1, 2, 3]) 
tensor([[[1, 2, 3],
         [4, 5, 6]]])
         
torch.Size([2, 1, 3]) 
tensor([[[1, 2, 3]],
        [[4, 5, 6]]])
        
torch.Size([2, 3, 1]) 
tensor([[[1],
         [2],
         [3]],
        [[4],
         [5],
         [6]]])
```
**缩减维度**
使用tensor的squeeze方法来缩减tensor的大小为1的维度。
可以指定需要缩减的维度索引
如果不指定，则会缩减所有大小为1的维度
```python
x = torch.ones((1,1,3))
print(x.shape, x)
y = x.squeeze(dim=0)
print(y.shape, y)
z = x.squeeze()
print(z.shape, z)
```
```output
torch.Size([1, 1, 3]) tensor([[[1., 1., 1.]]])
torch.Size([1, 3]) tensor([[1., 1., 1.]])
torch.Size([3]) tensor([1., 1., 1.])
```

**统计函数**
`tensor.sum()`求和
`tensor.mean()`求均值
`tensor.std()`求标准差
`tensor.min()`求最小值

Tensor指定统计的维度，意味着要“消灭”这个维度。比如指定`dim=0`意味着，统计结果要“消灭”行这个维度，各个列的值在不同行上进行统计。
![[Pasted image 20251014170326.png]]
因此对于t1，如果使用t1.meam(dim=0)，有mean = \[1,3]

**索引和切片**
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(x[0, 1])  # 访问第一行第二个元素
print(x[:, 1])  # 访问第二列
print(x[1, :])  # 访问第二行
print(x[:, :2])  # 访问前两列
```
**广播机制的一般原则是：**
**维度对齐**
先检查两个tensor的形状，如果它们的维度个数不同，**在短的那个前边补1**，使它们的维度个数相同。 比如对tensor t1和t2进行加法：
**例1：**
t1的shape为（3,2,2）
t2是个标量，shape为空。
则通过reshape将t2的shape调整为（1,1,1）
**例2：**
t1的shape为（2,2）
t2的shape为（3,2,2）
则通过reshape将t1的shape调整为（1,2,2）

**扩展维度**
在维度值为1的维度上，通过虚拟复制，让两个tensor的维度值相等。 对于上一步维度对齐后的例子分别有：
**例1：**
t1的shape为（3,2,2）
t2的shape为（1,1,1）
扩展t2的维度为（3,2,2）
**例2：**
t1的shape为（1,2,2）
t2的shape为（3,2,2）
扩展t1的维度为（3,2,2）

**检查是否有可用cpu**
```python
torch.cuda.is_available()
```