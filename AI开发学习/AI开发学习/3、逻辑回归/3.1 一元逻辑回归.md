![[Pasted image 20251014205834.png]]
对于如此问题，sigmoid天然能进行二分类
$$LogisticRegssion(x)=sigmoid(x)=\frac{1}{1+e^{−wx+b}}​$$
当你有一个训练好的逻辑回归模型，它的输出是Sigmoid函数的输出，范围是0-1之间，正好是概率值的范围。
越接近1，证明这个样本越接近正例。
一般默认以0.5作为决策临界值。
大于等于0.5则认为是正例，
小于0.5则认为是负例。
对Sigmoid函数的输入x，增加了w和b参数。实际上就是对x应用了一个线性回归，然后再进行Sigmoid变化。

**损失函数**
训练一个模型，需要定义损失函数，利用梯度下降算法，让损失越来越小
![[Pasted image 20251014210114.png]]
利用均方误差会发现其是一个非凸函数，稳定性差，因此需要使用其他方法。

**交叉熵损失函数**
$$BCELoss = -log(\hat y) \tag {y=1}$$
$$BCELoss = -log(1-\hat y) \tag {y=0}$$
因此有$$BCELoss(y,\hat y) = -[ylog\hat y + (1-y)log(1-\hat y)]$$




