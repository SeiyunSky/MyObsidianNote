### L1正则化
L1正则化的思想是在损失函数中额外加入模型所有参数的绝对值之和作为惩罚项。这样模型在训练过程中，会尽可能的让参数变小来降低Loss值，直到让某些参数为0。
如果一个参数降为0，则等价于这个参数消失，起到了降低模型复杂度的作用。同样减小参数的值也是降低模型复杂度的一种方式。
L1正则化的公式为：
$$ \text{Loss}_{L1} = \text{Loss}(\theta) + \lambda\sum_{i}|\theta_{i}| $$

由于新增项绝对值梯度为1，0，-1三个离散点，因此最终会导致模型大量参数为0。

### L2正则化
$$ \text{Loss}_{L1} = \text{Loss}(\theta) + \lambda\sum_{i}\theta_{i}^2 $$
L2采用的平方，平方项的导数为2x，因此会相对平滑的产生梯度变化，参数会逐步接近0，但不会直接等于0。

**添加L1或L2正则化的方法**
通常情况下不使用L1正则化，会直接消除部分参数的意义，L2正则化能够保留特征贡献并削弱部分特征的强依赖性，常用。
```python
l1_norm = 0.0
for param in model.parameters():
      l1_norm += param.abs().sum()
loss = criterion(outputs, labels) + 1e-4 * l1_norm

l2_norm = 0.0
for param in model.parameters():
      l2_norm += param.pow(2).sum()
loss = criterion(outputs, labels) + 1e-4 * l2_norm
```