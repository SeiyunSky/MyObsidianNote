### 权重衰减的原理
权重衰减的思想很简单，就是在训练的每一步用梯度更新参数时，同时缩小参数值。防止参数的绝对值过大。权重衰减的思想和L1、L2正则是类似的，都是减少参数的绝对值。不同的是L1、L2正则是在loss函数里增加额外项实现的。而权重衰减的做法更直接，直接减小参数的绝对值。
$$w_t​=w_t​−lr×g_w​−lr×λw_t​$$
```python
optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
```

权重衰减对于标准梯度下降算法来说效果和L2正则化是一样的。