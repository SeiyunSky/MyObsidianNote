![[Pasted image 20251016135028.png]]
**利用矩阵计算实现神经网络计算的加速**
![[Pasted image 20251016135250.png]]

**激活函数**
没有激活函数，不论几层的神经网络都是一个线性回归。激活函数的作用是引入非线性。
![[Pasted image 20251016135342.png]]
对于此图，如果没有激活函数，那么无论有多少层线性回归，最终都只等价于一层的线性回归。
![[Pasted image 20251016135424.png]]

**常用激活函数**
$$sigmoid(x)=\frac {1}{1+e^{−x}}$$
$$tanh(x)=\frac {e^x+e^{−x}}{e^x−e^{−x}}​$$
$$softmax = \frac {e^i}{\sum_{j=1}^n e^j}$$​
$$ReLU(x)=max(x,0)$$
Relu目前是深度学习中最默认的激活函数，但在x小于0的情况下，导致其无法更新参数，因此有,让x为负数的情况下产生微小梯度
$$LeakyReLu(x) = x \tag {x>0}$$
$$LeakyReLu(x) = ax \tag {x<0}$$

