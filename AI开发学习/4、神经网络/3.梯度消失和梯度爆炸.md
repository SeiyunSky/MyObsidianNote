![[Pasted image 20251016142052.png]]
**激活函数的选择**
因此，对于深度神经网络中，需要选择梯度不会进行大幅度偏移的激活函数作为隐藏层的激活函数，sigmoid和tanh的梯度显然不满足，而Relu能够保证梯度恒为1或0，因此适合作为隐藏层，以避免梯度消失与梯度爆炸。

**参数值的初始化**
- **消除对称性**
  首先参数初始化一定要随机，如果某一层的神经元的参数都设置为同样的初始值。因为对于同一层的不同神经元，它们的输入是相同的，如果参数也相同，则输出也相同，反向传播的梯度也相同，每次参数更新后的值也一样，多个神经元就失去了差异性。所以初始化参数时一定要随机，来消除神经元之间的对称性。

- **控制方差**
  一般神经网络的输入都经过了标准化，输入都是均值为0，方差为1的变量。我们可以通过随机生成均值为0，方差为1的参数值来初始化网络参数。但是有个问题，就是我们希望神经网络正向传播时，每一层输出的方差不变，保持稳定。但是神经网络每一层都是每个输入和对应权重乘积的累加，有n个输入，就是n个值的累加。导致方差变为n。另外考虑因为采用ReLU激活函数，导致大约一半的输入为0，所以方差应该修正为n22n​，为了保持方差不变，对初始化权重进行调整，将权重初始化为均值为0，标准差为2nn2​​的随机变量。其中n为当前层输入的数量。这是用正态分布初始化参数的方法，也有用平均分布来初始化权重的方法。对于偏置，默认初始化为全零即可。不用担心，在使用PyTorch定义神经网络层时，它会自动帮你初始化好参数。

