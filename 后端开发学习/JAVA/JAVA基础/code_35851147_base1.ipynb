{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad5375c6-80bb-4afd-ad1f-082db4388f7b",
      "metadata": {
        "id": "ad5375c6-80bb-4afd-ad1f-082db4388f7b"
      },
      "source": [
        "## Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5b0e14cd-8ec7-48ec-b621-3d014f3a864a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b0e14cd-8ec7-48ec-b621-3d014f3a864a",
        "outputId": "4a9bfb1b-296b-4910-d367-ae9fd043f3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/My Drive/FIT5217 Assignment2/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64436517-7067-4593-b84a-10c080d17ef4",
      "metadata": {
        "id": "64436517-7067-4593-b84a-10c080d17ef4"
      },
      "source": [
        "## Load DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c5a95a24-da8c-4a0d-89d3-429cd58af0a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a95a24-da8c-4a0d-89d3-429cd58af0a6",
        "outputId": "454f8e57-7894-40a8-de12-7ee414ffdf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 162899 training pairs\n",
            "Trimmed to 158251 training pairs (sequences shorter than 100 tokens)\n",
            "\n",
            "Building vocabulary from training data...\n",
            "Counted words:\n",
            "- ingredients: 8532 words\n",
            "- recipe: 12543 words\n",
            "\n",
            "Example training pair:\n",
            "[' c . butter or margarine c . powdered sugar c . milk tsp . vanilla use clear vanilla for a white frosting ', ' put butter in bowl and beat with mixed until fluffy . add cups of the powdered sugar then beat in the c . milk and vanilla . add remaining sugar and mix until smooth and fluffy . ']\n"
          ]
        }
      ],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', str(s))\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    ).lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData():\n",
        "    train_df = pd.read_csv(file_path + 'train.csv')\n",
        "    dev_df = pd.read_csv(file_path + 'dev.csv')\n",
        "    test_df = pd.read_csv(file_path + 'test.csv')\n",
        "\n",
        "    train_pairs = [[normalizeString(row['Ingredients']), normalizeString(row['Recipe'])] for index, row in train_df.iterrows()]\n",
        "    dev_pairs = [[normalizeString(row['Ingredients']), normalizeString(row['Recipe'])] for index, row in dev_df.iterrows()]\n",
        "    test_pairs = [[normalizeString(row['Ingredients']), normalizeString(row['Recipe'])] for index, row in test_df.iterrows()]\n",
        "\n",
        "    print(\"Read %s training pairs\" % len(train_pairs))\n",
        "    train_pairs = filterPairs(train_pairs)\n",
        "    print(\"Trimmed to %s training pairs (sequences shorter than %d tokens)\" % (len(train_pairs), MAX_LENGTH))\n",
        "\n",
        "    input_lang = Lang('ingredients')\n",
        "    output_lang = Lang('recipe')\n",
        "\n",
        "    print(\"\\nBuilding vocabulary from training data...\")\n",
        "    for pair in train_pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(f\"- {input_lang.name}: {input_lang.n_words} words\")\n",
        "    print(f\"- {output_lang.name}: {output_lang.n_words} words\")\n",
        "\n",
        "    return input_lang, output_lang, train_pairs, dev_pairs, test_pairs\n",
        "\n",
        "input_lang, output_lang, train_pairs, dev_pairs, test_pairs = prepareData()\n",
        "\n",
        "# Print a random training pair to verify\n",
        "print(\"\\nExample training pair:\")\n",
        "print(random.choice(train_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6678b1-1256-4d21-a130-008e1668ed92",
      "metadata": {
        "id": "8d6678b1-1256-4d21-a130-008e1668ed92"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ba2c109-8caf-4d04-872e-0af5ffc81440",
      "metadata": {
        "id": "6ba2c109-8caf-4d04-872e-0af5ffc81440"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        '''\n",
        "        GRU is a gated RNN variant that captures long-term dependencies more effectively with fewer parameters. GRU has the same output shape as a standard RNN when configured identically.\n",
        "        '''\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc51ebc5-93a4-41e6-ad45-d99dd11e72a3",
      "metadata": {
        "id": "dc51ebc5-93a4-41e6-ad45-d99dd11e72a3"
      },
      "source": [
        "## Prepare Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24869316-b87a-4025-8a35-6a7ab6ed2fe0",
      "metadata": {
        "id": "24869316-b87a-4025-8a35-6a7ab6ed2fe0"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d124229-39d8-4fe9-9205-84648b3e5476",
      "metadata": {
        "id": "1d124229-39d8-4fe9-9205-84648b3e5476"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "158d665c-1ff2-45be-bb9e-f650d2c4b6e9",
      "metadata": {
        "id": "158d665c-1ff2-45be-bb9e-f650d2c4b6e9"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 100 # Maximum sentence length to consider\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfe3cfb-4ea0-487b-bb44-58653b56ed5a",
      "metadata": {
        "id": "cbfe3cfb-4ea0-487b-bb44-58653b56ed5a"
      },
      "source": [
        "## Show Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "440ec559-bb15-4f0c-9e54-76bb01851bef",
      "metadata": {
        "id": "440ec559-bb15-4f0c-9e54-76bb01851bef"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e6905a-cae7-45c8-ac10-d71e6ec271a9",
      "metadata": {
        "id": "a6e6905a-cae7-45c8-ac10-d71e6ec271a9"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2344ea14-4248-4b22-904f-d855c71e349c",
      "metadata": {
        "id": "2344ea14-4248-4b22-904f-d855c71e349c"
      },
      "outputs": [],
      "source": [
        "# This function runs the trained encoder and decoder on a given input sentence to generate an output sequence.\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    # We don't need gradients during evaluation, so we disable them for efficiency\n",
        "    with torch.no_grad():\n",
        "        # Convert the input sentence (a string) into a tensor of token indices\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        # Initialize the encoder's hidden state\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        # Create a tensor to store encoder outputs at each time step (for attention, if used)\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Pass each token of the input through the encoder one at a time\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]  # Store encoder output\n",
        "\n",
        "        # Decoder starts with the <SOS> (start-of-sequence) token\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "        # Decoder's initial hidden state is the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # List to store the output words predicted by the decoder\n",
        "        decoded_words = []\n",
        "\n",
        "        # Tensor to store attention weights (not used here, but often included for attention visualisation)\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        # Generate words one by one until we reach max_length or predict <EOS>\n",
        "        for di in range(max_length):\n",
        "            # Pass current input and hidden state to the decoder\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Get the most likely next word (highest probability)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            # If decoder predicts <EOS>, stop decoding\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                # Convert predicted index to actual word and append to output sequence\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            # Use the predicted word as the next input to the decoder (greedy decoding)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        # Return the list of predicted words as the final output\n",
        "        return decoded_words\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words= evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8030a4e2-2f7e-40b8-baa8-9f0043e9b563",
      "metadata": {
        "id": "8030a4e2-2f7e-40b8-baa8-9f0043e9b563"
      },
      "source": [
        "## Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1a38a5b0-32dd-46f5-9680-18c8ed7bac6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a38a5b0-32dd-46f5-9680-18c8ed7bac6a",
        "outputId": "606db33a-eeca-45bf-dcb0-4432ef78a63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 25s (- 21m 46s) (2000 10%) 9.2339\n",
            "4m 44s (- 18m 58s) (4000 20%) 12.3760\n",
            "7m 5s (- 16m 32s) (6000 30%) 13.6873\n",
            "9m 26s (- 14m 9s) (8000 40%) 13.8575\n",
            "11m 46s (- 11m 46s) (10000 50%) 13.2346\n",
            "14m 4s (- 9m 23s) (12000 60%) 12.7773\n",
            "16m 29s (- 7m 4s) (14000 70%) 13.7940\n",
            "18m 52s (- 4m 43s) (16000 80%) 13.0565\n",
            "21m 11s (- 2m 21s) (18000 90%) 12.5538\n",
            "23m 30s (- 0m 0s) (20000 100%) 11.4739\n"
          ]
        }
      ],
      "source": [
        "# Set the size of the hidden layers in both the encoder and decoder RNNs\n",
        "hidden_size = 256\n",
        "\n",
        "# Create an instance of the encoder RNN\n",
        "# It takes input_lang.n_words as the input vocabulary size (i.e., number of unique input tokens)\n",
        "# and hidden_size as the size of the RNN's hidden state\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "\n",
        "# Create an instance of the decoder RNN\n",
        "# It takes hidden_size from the encoder and output_lang.n_words as the output vocabulary size\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# Set the total number of training iterations (i.e., how many sentence pairs to train on)\n",
        "n_iters = 20000\n",
        "\n",
        "# Set how often to print the training loss\n",
        "print_every = 2000\n",
        "\n",
        "plot_losses = trainIters(encoder1, decoder1, n_iters=n_iters, print_every=print_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6f3b58-9336-4850-a206-e5ee84ec9748",
      "metadata": {
        "id": "2b6f3b58-9336-4850-a206-e5ee84ec9748"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}