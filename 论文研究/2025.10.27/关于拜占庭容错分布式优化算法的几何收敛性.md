**ON THE GEOMETRIC CONVERGENCE OF BYZANTINE-RESILIENT DISTRIBUTED OPTIMIZATION ALGORITHMS**
[查看原文](file:///C:/Users/admin/Desktop/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/Week%206/On%20the%20geometric%20convergence%20of%20Byzantine-resilie.pdf)

### **论文贡献**
(i) 引入一个名为 **REDGRAF** 的算法框架，它是**对 中 BRIDGE 算法**的推广，并将一些最先进的拜占庭弹性分布式优化算法作为特例包含在内。  
(ii) 我们提出了一个新颖的收缩性质，为证明 REDGRAF 框架中算法的**几何收敛性**提供了.一种通用方法。据我们所知，这是首个在强凸性条件下，为弹性算法证明所有常规代理状态能够以几何速率收敛到一个包含真实最小化器的球的工作。我们还明确地描述了收敛速率和收敛区域的大小。  
(iii) 我们引入了一个新颖的混合动态性质，用于推导 REDGRAF 框架中算法的近似共识结果，并明确描述了收敛速率和最终共识直径。  
(iv) 利用我们的框架，我们分析了一些最先进算法的收缩和混合动态性质，从而为每种算法得出了收敛和共识结果。我们的工作首次证明了这些算法满足此类性质。  
(v) 我们通过数值模拟来展示和比较这些算法的性能，以验证关于收敛和近似共识的理论结果。

| Algorithm        | Function Class                | Step Size   | Convergence Rate              | Convergence Region         | Network Condition |
| ---------------- | ----------------------------- | ----------- | ----------------------------- | -------------------------- | ----------------- |
| ​**​Our Work​**​ | Strongly Convex               | Constant    | ​**​Linear​**​                | Neighborhood of x∗         | Robustness        |
| [35], [33], [34] | Convex                        | Diminishing | Asymptotic                    | Convex Hull of xi∗​        | Robustness        |
| [20], [21]       | Convex                        | Diminishing | Asymptotic                    | Neighborhood of x∗         | Robustness        |
| [39]             | Strongly Convex (i.i.d. data) | Diminishing | Asymptotic (high probability) | Exact x∗(high probability) | Robustness        |
| [7]              | Strongly Convex (i.i.d. data) | Diminishing | Asymptotic (high probability) | Exact x∗(high probability) | Robustness        |
| [11]             | Convex (redundancy)           | Constant    | Finite-time                   | Exact x∗                   | Complete Graph    |

考虑一个由 N 个代理组成的群体 V，它们通过一个图 $G = (V, E)$ 相互连接。每个代理 vi ∈ V 都有一个局部成本函数$fi : Rᵈ → R$。由于拜占庭节点被允许在任何算法的每次迭代中向其邻居发送任意值，因此不可能最小化通常在分布式优化中寻求的量 $Σᵥᵢ∈ᵥ f_i(x)$（因为无法保证能推断出关于拜占庭代理真实函数的任何信息）。因此，我们将求和仅限于常规代理的函数，即，目标是解决以下优化问题，
$$min f(x),  f(x) := (\frac {1}{|V_ℛ|}) Σ_{V_i∈V_ℛ} f_i(x)  \tag {3.3}$$$V_R$​代表常规代理的集合。
在缺乏对局部函数通用假设（即不假设独立同分布或冗余性）的情况下，为弹性分布式优化算法建立收敛性（尤其是获得收敛速率）并非易事。通过引入一个适当的中间步骤来弥补这一空白，即在**证明收敛性（命题5.7和定理5.8）之前先证明状态收缩性质（定义5.4）**。

### **弹性分布式梯度下降算法框架**
输入网络、局部成本函数（常规智能体各自拥有的目标函数的集合），参数F（局部模型下邻居最多可能存在的拜占庭代理数量）
**步骤一**：初始化
**步骤二**：广播与接收，节点将其状态广播给所有出邻居，并接收所有入邻居的状态
**步骤三**：过滤
![[Pasted image 20251027170840.png|170]]
（这里产生的$z_i[k]$是由中间向量和辅助向量组合得到的）
**步骤四**：梯度更新
![[Pasted image 20251027170920.png|260]]
**对于所有常规节点**：
![[Pasted image 20251027170953.png|230]]
（这里的x是全局可信的向量，y是本身局部更新的辅助向量）
![[Pasted image 20251027165138.png|600]]

| 字母/符号                 | 含义与解释                                                                                                                                          |
| --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| ​**​REDGRAF​**​       | ​**​Res​**​ilient ​**​D​**​istributed ​**​G​**​radient-​**​D​**​escent ​**​A​**​lgorithmic ​**​F​**​ramework（弹性分布式梯度下降算法框架）的缩写。这是本文提出的统一框架的名称。 |
| G                     | 代表通信​**​网络拓扑​**​，通常是一个有向图 G=(V,E)，其中 V是节点集，E是边集。                                                                                               |
| $V_R​$                | 常规代理的集合。即网络中诚实、遵循算法的“好”节点的集合。下标 R代表 Regular。                                                                                                   |
| $\{f_i​\}_{V_R}​​$    | 所有常规代理的​**​局部成本函数​**​的集合。每个代理$v_i$​有自己的目标函数 $f_i​:R_d→R$。优化目标是最小化这些函数的平均值。                                                                     |
| F                     | ​**​容错参数​**​。它定义了“F-局部”故障模型：任何常规代理最多有 F个拜占庭入邻居。它是过滤步骤中决定移除多少可疑值的关键参数。                                                                          |
| $v_i​$                | 第 i个代理（节点）。                                                                                                                                    |
| $z_i​[k]$             | 代理 vi​在时刻（迭代次数）k的​**​完整状态向量​**​。它是一个组合向量，zi​[k]=[xiT​[k],yiT​[k]]T。                                                                            |
| $x_i​[k]$             | 代理 vi​在时刻 k的​**​主状态向量​**​。这是对优化问题解 x∗的估计，是算法的核心输出。                                                                                             |
| $y_i​[k]$             | 代理 vi​在时刻 k的​**​辅助向量​**​。某些算法（如 SDMMFD）使用它来帮助过滤恶意信息。它是一个可选状态。                                                                                  |
| $init(f_i​)$          | ​**​初始化函数​**​。根据局部函数 fi​初始化状态 zi​[0]。例如，可以将 yi​[0]设置为 fi​的一个近似极小值点。                                                                            |
| k                     | 迭代​**​时间步​**​或​**​轮次​**​的索引，k=0,1,2,...。                                                                                                       |
| $N_i^{out}​$          | 代理 vi​的​**​出邻居​**​集合。即 vi​可以发送信息给这些邻居。                                                                                                         |
| $N_i^{in}$​           | 代理 vi​的​**​入邻居​**​集合。即 vi​可以从这些邻居接收信息。                                                                                                         |
| $Z_i​[k]$             | 在时刻 k，代理 vi​可访问的​**​所有状态向量的集合​**​。包括它自己的状态 zi​[k]以及从所有入邻居接收到的状态。                                                                               |
| $filt(⋅)$             | ​**​过滤函数​**​。这是实现拜占庭容错能力的核心。它利用参数 F，从接收到的数据集 Zi​[k]中识别并移除潜在的恶意或不可靠值，输出一个“净化”后的中间状态 z~i​[k]。                                                    |
| $\hat z~_i​[k]$       | 经过过滤步骤后得到的​**​中间状态向量​**​，![[Pasted image 20251027170727.png\|100]]。                                                                            |
| $\hat x~_i​[k]$       | 过滤后得到的​**​中间主状态向量​**​。它将用于后续的梯度更新。                                                                                                             |
| $\hat y~_i​[k]$       | 过滤后得到的​**​中间辅助向量​**​。                                                                                                                          |
| $α_k​$                | 第 k次迭代的​**​步长​**​（学习率）。αk​∈R>0​是一个正实数。论文主要分析恒定步长 α的情况。                                                                                         |
| $∇f_i​(\hat x_i​[k])$ | 局部函数 fi​在点的​**​梯度​**​。它指示了函数在该点处最陡下降的方向。                                                                                                       |
### **定义弹性分布式优化的若干标准操作​**
- **过滤操作​**​：用于从接收到的数据中识别并移除潜在的恶意值。
    - `dist_filt`：基于与“信任锚点”$y_i​[k]$的​**​整体距离​**​进行过滤。
    - `full_mm_filt`：基于​**​整个向量​**​在每个维度上的值范围进行过滤。
    - `cw_mm_filt`：​**​按每个坐标维度​**​分别进行过滤。      
- ​**​聚合操作​**​：对过滤后保留的“可信”值进行聚合，以计算出一个安全的中间值。
    - `full_average`：对​**​完整状态向量​**​进行加权平均。
    - `cw_average`：​**​按每个坐标维度​**​分别进行加权平均。
    - `safe_point`：保证输出是​**​已知正常代理状态​**​的凸组合。(FLTrust)
![[Pasted image 20251027202857.png]]![[Pasted image 20251027202912.png]]

**文章假设和主要结果**
5.1小节中陈述用于在第5.2小节证明收敛性以及在5.3小节证明一致性属性的假设和定义

5.1 **假设与定义**：
![[Pasted image 20251028202028.png]]

![[Pasted image 20251028202521.png]]![[Pasted image 20251028202529.png]]![[Pasted image 20251028202534.png]]

**状态收敛的区域**
![[Pasted image 20251028205038.png]]
![[Pasted image 20251028205118.png]]
![[Pasted image 20251028205132.png]]
![[Pasted image 20251028205148.png]]![[Pasted image 20251028205159.png]]
