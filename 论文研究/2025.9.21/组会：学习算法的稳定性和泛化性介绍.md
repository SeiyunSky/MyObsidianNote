Train faster, generalize better: Stability of stochastic gradient descent
这篇论文在干的事情是，用SGD方法，基于下述定理，通过计算验证，可以得到一个合理的跑偏上界，且越快越说明SGD方法越成功。

核心问题在于——假设太强了、基于自监督学习很难用这套理论去进行计算证明
 **є-均匀稳定**
 这个东西有多小，那么最终的期望泛化误差$\left| \mathbb{E}_{S, A}\left[ R_{S}[A(S)] - R[A(S)] \right] \right| \leq \epsilon$就有多小
![[Pasted image 20250914233007.png|400]]
其中的sup指的是上确界，能取到的最大值，而S‘和S是有一定差异的数据集

 
**n-扩张性** **-有界性**
G是移动后的两个点，这里看的是移动前后两个点的比值关系
![[Pasted image 20250914233338.png|400]]
- η > 1: G会让点互相“远离”，是“扩张”的。
- η = 1: G能保持距离不变，是“非扩张”的。
- η < 1: G会让点互相“靠近”，是“收缩”的

 如果变化前后差值永远有上界，那么变化就是有界的

**增长递归**
 说明两个有一个差别的数据集在训练过程中的欧几里得距离最大上界
![[Pasted image 20250914233942.png|300]]


1. **权重衰减 (Weight Decay) / L2正则化**:
    - **它做什么？** 论文证明，加入权重衰减，相当于在我们的“扩张性”指标η里，减去了一个常数。它让那个可能大于1的η，变得更小，甚至直接小于1。
    - **一句话总结**: 权重衰减是一个**强效的“收缩剂”**，它帮助SGD的并行世界更快地“靠拢”，从而提升稳定性。
        
2. **梯度裁剪 (Gradient Clipping)**:
    - **它做什么？** 它粗暴地给每一步的梯度大小设置一个上限
    - **一句话总结**: 梯度裁剪是一个**“限速器”**，它减小了“分道扬镳”时注入的差异量，从而提升稳定性。
        
3. **Dropout**:
    - **它做什么？** 论文证明，Dropout这种随机丢弃神经元的行为，从数学效果上看，相当于**同时减小了“扩张性”η和“步长上界”σ**。它既是“收缩剂”又是“限速器”。
    - **一句话总结**: Dropout是一个**终极的“稳定器”**，它从两个维度同时加强了SGD的稳定性。

---


