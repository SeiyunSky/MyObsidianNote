### 修改逻辑解析
*   **原 BALANCE**:
    *   阈值 $T = \gamma \cdot e^{-\kappa t} \cdot \|w_{own}\|$
    *   $\gamma$ 是一个固定的标量（如 0.3），意味着无论对方方向如何，只要距离足够近就接受。
*   **修改后 (Hybrid)**:
    *   我们将 $\gamma$ 视为**“信任系数”**。
    *   使用 FLTrust 的核心思想：**信任系数由方向一致性决定**。
    *   动态阈值 $T_{neighbor} = \text{ReLU}(\cos(\theta)) \cdot e^{-\kappa t} \cdot \|w_{own}\|$


### 项目结构

```text
dfl_experiment/
├── config.py           # 实验参数配置
├── utils.py            # 工具：模型向量化、距离/范数计算
├── models.py           # CNN模型定义
├── attacks.py          # 攻击逻辑 (后门触发器、高斯噪声)
├── data_loader.py      # 数据加载与切分 (含Label Flipping)
├── aggregators.py      # 聚合算法 (FedAvg, BALANCE, BALANCE_Modified)
├── client.py           # 客户端定义 (正常/恶意)
└── main.py             # 主运行脚本与可视化
```

---

### 1. `config.py` (配置参数)
```python
# config.py
PARAMS = {
    'num_clients': 20,
    'num_neighbors': 10,       # 拓扑中每个节点的邻居数
    'num_malicious': 4,        # 恶意节点数
    'rounds': 200,             # 训练轮次 (演示用200，论文复现建议1000+)
    'learning_rate': 0.01,
    'alpha': 0.5,              # 本地模型与聚合模型的融合权重
    
    # 原始 BALANCE 参数
    'gamma': 0.5,              # 固定阈值系数
    'kappa': 1.0,              # 时间衰减系数
    
    # 攻击相关参数
    'backdoor_target_label': 7,
    'gauss_scale': 0.5,        # 高斯噪声强度
    'trim_scale': -2.0         # Trim攻击缩放比例
}
```

### 2. `utils.py` (工具函数)
```python
# utils.py
import torch

def model_to_vector(state_dict):
    """将模型参数扁平化为一维向量，用于计算余弦相似度"""
    vec = []
    # 必须排序 keys 以保证向量顺序一致
    for key in sorted(state_dict.keys()):
        # 只处理权重和偏置
        if 'weight' in key or 'bias' in key:
            vec.append(state_dict[key].float().view(-1))
    return torch.cat(vec)

def get_model_norm(model_state_dict):
    """计算模型参数的 L2 范数 (用于 BALANCE 阈值计算)"""
    norm = 0.0
    for key, param in model_state_dict.items():
        if 'weight' in key or 'bias' in key:
            norm += torch.norm(param.float())**2
    return torch.sqrt(norm).item()

def get_model_dist(model1, model2):
    """计算两个模型之间的欧氏距离"""
    dist = 0.0
    for key in model1:
        if 'weight' in key or 'bias' in key:
            dist += torch.norm(model1[key].float() - model2[key].float())**2
    return torch.sqrt(dist).item()
```

### 3. `aggregators.py` (【核心】聚合算法)
这里包含了你要的 **Modified BALANCE**。

```python
# aggregators.py
import torch
import numpy as np
from collections import OrderedDict
from utils import model_to_vector, get_model_norm, get_model_dist

def aggregate_fedavg(neighbor_models_states):
    """标准 FedAvg"""
    if not neighbor_models_states:
        return None
    avg_state = OrderedDict()
    for key in neighbor_models_states[0]:
        avg_state[key] = torch.stack([s[key].float() for s in neighbor_models_states]).mean(0)
    return avg_state

def aggregate_balance_original(own_state, neighbor_states, gamma, kappa, t_ratio):
    """
    原始 BALANCE 算法:
    Threshold = gamma * exp(-kappa * t) * ||own||
    gamma 是固定的标量。
    """
    own_norm = get_model_norm(own_state)
    threshold = gamma * np.exp(-kappa * t_ratio) * own_norm
    
    accepted_states = []
    for n_state in neighbor_states:
        dist = get_model_dist(own_state, n_state)
        if dist <= threshold:
            accepted_states.append(n_state)
            
    if not accepted_states:
        return None
    return aggregate_fedavg(accepted_states)

def aggregate_balance_modified(own_state, neighbor_states, kappa, t_ratio):
    """
    【你的需求实现】修改版 BALANCE:
    将固定参数 gamma 替换为动态的 ReLU(Cosine_Similarity)。
    
    逻辑:
    1. 计算 CosSim(Own, Neighbor)。
    2. dynamic_gamma = max(0, CosSim)。
    3. Threshold = dynamic_gamma * exp(-kappa * t) * ||own||。
    4. 如果 dist <= Threshold，则接受。
    """
    own_vec = model_to_vector(own_state)
    own_norm = get_model_norm(own_state)
    
    # 时间衰减项 (Common Term)
    decay = np.exp(-kappa * t_ratio)
    
    accepted_states = []
    
    for n_state in neighbor_states:
        n_vec = model_to_vector(n_state)
        
        # --- 核心替换 Start ---
        # 1. 计算余弦相似度
        cos_sim = torch.nn.functional.cosine_similarity(own_vec, n_vec, dim=0).item()
        
        # 2. 动态 Gamma (FLTrust 核心思想: 只有正向相似才信任)
        # 如果方向相反 (cos < 0)，dynamic_gamma = 0 -> 阈值 = 0 -> 直接拒绝
        dynamic_gamma = max(0.0, cos_sim)
        
        # 3. 计算该邻居的专属阈值
        threshold = dynamic_gamma * decay * own_norm
        # --- 核心替换 End ---
        
        # 4. 距离判定 (保留 BALANCE 的距离截断逻辑)
        dist = torch.norm(own_vec - n_vec).item()
        
        if dist <= threshold:
            accepted_states.append(n_state)
            
    if not accepted_states:
        return None
        
    return aggregate_fedavg(accepted_states)
```

### 4. `models.py` (模型)
```python
# models.py
import torch.nn as nn
import torch.nn.functional as F

class CNN_MNIST(nn.Module):
    def __init__(self):
        super(CNN_MNIST, self).__init__()
        # 结构: Conv -> Pool -> Conv -> Pool -> FC -> Softmax
        self.conv1 = nn.Conv2d(1, 30, kernel_size=3)
        self.conv2 = nn.Conv2d(30, 50, kernel_size=3)
        self.fc1 = nn.Linear(1250, 100)
        self.fc2 = nn.Linear(100, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(-1, 1250)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)
```

### 5. `attacks.py` (攻击函数)
```python
# attacks.py
import torch

def add_backdoor_trigger(data, target_label):
    """在MNIST图像右下角添加白色方块"""
    triggered_data = data.clone()
    # 28x28 图像，在右下角 26:28 区域置为最大值
    triggered_data[:, :, 26:28, 26:28] = triggered_data.max()
    
    targets = torch.full((data.shape[0],), target_label, dtype=torch.long)
    return triggered_data, targets

def apply_gaussian_noise(state_dict, scale=0.1):
    new_state = {}
    for key, param in state_dict.items():
        new_state[key] = param + torch.randn_like(param) * scale
    return new_state

def apply_trim_attack(state_dict, scale=-2.0):
    new_state = {}
    for key, param in state_dict.items():
        new_state[key] = param * scale
    return new_state
```

### 6. `client.py` (客户端逻辑)
```python
# client.py
import torch
import torch.optim as optim
import torch.nn.functional as F
from collections import OrderedDict
from attacks import add_backdoor_trigger, apply_gaussian_noise, apply_trim_attack

class Client:
    def __init__(self, client_id, model, train_loader, lr):
        self.client_id = client_id
        self.model = model
        self.train_loader = train_loader
        self.lr = lr

    def local_train(self):
        optimizer = optim.SGD(self.model.parameters(), lr=self.lr)
        self.model.train()
        # 简单训练 1 个 epoch
        for data, target in self.train_loader:
            optimizer.zero_grad()
            output = self.model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
        return self.model.state_dict()

class MaliciousClient(Client):
    def __init__(self, client_id, model, train_loader, lr, attack_config):
        super().__init__(client_id, model, train_loader, lr)
        self.attack_type = attack_config['attack_type']
        self.attack_config = attack_config

    def local_train(self):
        # 1. 正常训练基础 (Backdoor 需要特殊的数据处理，其他攻击是对权重的后处理)
        if self.attack_type == 'backdoor':
            return self._train_backdoor()
        
        # 2. 正常训练
        benign_state = super().local_train()
        
        # 3. 权重投毒
        if self.attack_type == 'gaussian':
            return apply_gaussian_noise(benign_state, self.attack_config['gauss_scale'])
        elif self.attack_type == 'trim':
            return apply_trim_attack(benign_state, self.attack_config['trim_scale'])
        else:
            return benign_state # Label Flipping 在 DataLoader 层处理，此处返回正常训练结果

    def _train_backdoor(self):
        optimizer = optim.SGD(self.model.parameters(), lr=self.lr)
        self.model.train()
        target_label = self.attack_config['backdoor_target_label']
        
        for data, target in self.train_loader:
            # 混合干净数据和投毒数据
            half = len(data) // 2
            clean_data, clean_target = data[:half], target[:half]
            
            # 生成触发器数据
            bad_data, bad_target = add_backdoor_trigger(data[half:], target_label)
            
            combined_data = torch.cat([clean_data, bad_data])
            combined_target = torch.cat([clean_target, bad_target])
            
            optimizer.zero_grad()
            output = self.model(combined_data)
            loss = F.nll_loss(output, combined_target)
            loss.backward()
            optimizer.step()
        return self.model.state_dict()
```

### 7. `data_loader.py` (数据)
```python
# data_loader.py
import torch
import numpy as np
from torchvision import datasets, transforms

def get_mnist_dataloaders(num_clients, malicious_ids, attack_type, non_iid_alpha=0.8):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)
    
    # Non-IID Dirichlet 划分
    labels = np.array(train_dataset.targets)
    label_indices = {i: np.where(labels == i)[0] for i in range(10)}
    client_indices = [[] for _ in range(num_clients)]
    
    for k in range(10):
        idx_k = label_indices[k]
        np.random.shuffle(idx_k)
        proportions = np.random.dirichlet(np.repeat(non_iid_alpha, num_clients))
        # 平衡切分防止越界
        proportions = np.array([p * (len(idx_k) < num_clients ? 1 : len(idx_k)) for p in proportions])
        proportions = (np.cumsum(proportions) * len(idx_k) / proportions.sum()).astype(int)[:-1]
        split_idx = np.split(idx_k, proportions)
        for i in range(num_clients):
            client_indices[i] += split_idx[i].tolist()
            
    loaders = []
    for i in range(num_clients):
        subset_indices = client_indices[i]
        
        # 处理 Label Flipping 攻击 (数据层)
        if i in malicious_ids and attack_type == 'label_flipping':
            data = train_dataset.data[subset_indices].float().unsqueeze(1) / 255.0
            data = (data - 0.1307) / 0.3081
            targets = train_dataset.targets[subset_indices].clone()
            targets = 9 - targets # 翻转标签
            dataset = torch.utils.data.TensorDataset(data, targets)
            loaders.append(torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True))
        else:
            sampler = torch.utils.data.SubsetRandomSampler(subset_indices)
            loaders.append(torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=sampler))
            
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)
    return loaders, test_loader
```

### 8. `main.py` (主程序)
```python
# main.py
import torch
import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict

from config import PARAMS
from models import CNN_MNIST
from data_loader import get_mnist_dataloaders
from client import Client, MaliciousClient
from aggregators import aggregate_fedavg, aggregate_balance_original, aggregate_balance_modified
from attacks import add_backdoor_trigger

def evaluate(model, test_loader, config):
    model.eval()
    
    # 1. Test Error Rate (TER)
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            pred = model(data).argmax(dim=1)
            correct += pred.eq(target).sum().item()
    ter = 1.0 - correct / len(test_loader.dataset)
    
    # 2. Attack Success Rate (ASR) - Only for Backdoor
    asr = 0.0
    if config['attack_type'] == 'backdoor':
        correct_trigger = 0
        total_trigger = 0
        target_label = config['backdoor_target_label']
        with torch.no_grad():
            for data, target in test_loader:
                mask = target != target_label
                if mask.sum() == 0: continue
                
                clean_data = data[mask]
                triggered_data, _ = add_backdoor_trigger(clean_data, target_label)
                
                pred = model(triggered_data).argmax(dim=1)
                correct_trigger += (pred == target_label).sum().item()
                total_trigger += len(triggered_data)
        asr = correct_trigger / total_trigger if total_trigger > 0 else 0
        
    return ter, asr

def run_simulation(algo_name, attack_name):
    print(f"\n=== Running: {algo_name} under {attack_name} ===")
    
    # 更新配置
    config = PARAMS.copy()
    config['algorithm'] = algo_name
    config['attack_type'] = attack_name
    
    # 准备数据
    malicious_ids = list(range(config['num_malicious'])) if attack_name != 'none' else []
    train_loaders, test_loader = get_mnist_dataloaders(
        config['num_clients'], malicious_ids, attack_name
    )
    
    # 建立拓扑 (环状)
    adj = np.zeros((config['num_clients'], config['num_clients']))
    for i in range(config['num_clients']):
        for j in range(1, config['num_neighbors']//2 + 1):
            adj[i, (i+j)%config['num_clients']] = 1
            adj[i, (i-j+config['num_clients'])%config['num_clients']] = 1

    # 初始化客户端
    clients = []
    global_init = CNN_MNIST()
    for i in range(config['num_clients']):
        model = CNN_MNIST()
        model.load_state_dict(global_init.state_dict())
        if i in malicious_ids:
            c = MaliciousClient(i, model, train_loaders[i], config['learning_rate'], config)
        else:
            c = Client(i, model, train_loaders[i], config['learning_rate'])
        clients.append(c)

    # 训练循环
    for t in range(config['rounds']):
        local_states = {}
        # 1. 本地训练
        for i in range(config['num_clients']):
            local_states[i] = clients[i].local_train()
            
        next_states = {}
        # 2. 聚合
        for i in range(config['num_clients']):
            neighbors = np.where(adj[i]==1)[0]
            neighbor_states = [local_states[n] for n in neighbors]
            own_state = local_states[i]
            
            agg_state = None
            if algo_name == 'FedAvg':
                agg_state = aggregate_fedavg(neighbor_states)
            elif algo_name == 'BALANCE':
                agg_state = aggregate_balance_original(
                    own_state, neighbor_states, 
                    config['gamma'], config['kappa'], t/config['rounds']
                )
            elif algo_name == 'BALANCE_Modified':
                # 注意：这里不再传入 gamma，因为内部用 ReLU(Cos) 动态计算
                agg_state = aggregate_balance_modified(
                    own_state, neighbor_states,
                    config['kappa'], t/config['rounds']
                )
            
            # 模型融合
            if agg_state:
                final = OrderedDict()
                for k in own_state:
                    final[k] = config['alpha'] * own_state[k] + (1-config['alpha']) * agg_state[k]
                next_states[i] = final
            else:
                next_states[i] = own_state
        
        # 3. 更新
        for i in range(config['num_clients']):
            clients[i].model.load_state_dict(next_states[i])
            
        if (t+1) % 50 == 0:
            print(f"Round {t+1} done.")

    # 最终评估 (取所有良性客户端的平均)
    benign_ters, benign_asrs = [], []
    for c in clients:
        if c.client_id not in malicious_ids:
            ter, asr = evaluate(c.model, test_loader, config)
            benign_ters.append(ter)
            benign_asrs.append(asr)
            
    return max(benign_ters), max(benign_asrs)

if __name__ == '__main__':
    # 比较三种算法在不同攻击下的表现
    scenarios = [
        # 格式: (攻击类型, 算法)
        ('label_flipping', 'FedAvg'),
        ('label_flipping', 'BALANCE'),
        ('label_flipping', 'BALANCE_Modified'),
        
        ('backdoor', 'FedAvg'),
        ('backdoor', 'BALANCE'),
        ('backdoor', 'BALANCE_Modified')
    ]
    
    results = {}
    for att, alg in scenarios:
        key = f"{alg}-{att}"
        ter, asr = run_simulation(alg, att)
        results[key] = {'ter': ter, 'asr': asr}
        print(f"Result {key}: TER={ter:.4f}, ASR={asr:.4f}")

    # 简单绘图
    labels = list(results.keys())
    ters = [results[k]['ter'] for k in labels]
    asrs = [results[k]['asr'] for k in labels]
    
    x = np.arange(len(labels))
    width = 0.35
    
    fig, ax = plt.subplots()
    ax.bar(x - width/2, ters, width, label='Test Error')
    ax.bar(x + width/2, asrs, width, label='Attack Success')
    
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right')
    ax.legend()
    plt.tight_layout()
    plt.savefig('result_comparison.png')
    plt.show()
```