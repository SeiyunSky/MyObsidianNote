### 1、讨论联邦学习下鲁棒性的维度
——引出本次汇报核心是分析拜占庭鲁棒性，给出基准目标
什么是拜占庭节点、拜占庭攻击的核心
存在拜占庭节点下的学习目标是什么
给出一个具体的拜占庭实例
![[Pasted image 20251001160544.png]]
参考文献：**Byzantine Machine Learning: A Primer**

——点出防御方法的大类
    两个方向——**排他、缓和**
    两种框架——**中心化、去中心化**

### 2、内容规划
#### 中心化
—— 首先从D-SGD 开始
![[Pasted image 20251001161911.png]]
参考文献：**Can Decentralized Algorithms Outperform Centralized Algorithms?**

—— 然后点出度量衡
核心考虑**残差误差**和**梯度复杂性**
参考文献：**Byzantine Machine Learning: A Primer**

—— 缓和 和 排他 的两个大类下的方法
几何、坐标中位数 （BGD)  /    Krum、CGE
参考文献：
**Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates**
**Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent**
**Byzantine Stochastic Gradient Descent**
**D-SGD and Norm-Based  Comparative Gradient Eliminatio**


—— Bulyan 和 FLtrust 综合考量两个大方向后的算法分析与利弊
参考文献：
**The Hidden Vulnerability of Distributed Learning in Byzantium**
**Fltrust:Byzantine-robust federated learning via trust bootstrapping**

—— 再来个框架式方法 ASKEL 
参考文献：**AK-SEL: Fast Byzantine SGD**

**过渡**：AKSEL 和 FLtrust 一方面违背了分布式下服务器不接触数据的问题、另外一方面，无法用在去中心化方法中

#### 去中心化
—— 进入去中心化方法的引子
    首先是**FedAvg**分析其在有攻击下的影响
参考文献：**Communication-Efficient Learning of Deep Networks from Decentralized Data**
    为什么需要重新设计、Krum/TrimMean的在去中心化下的问题
参考文献：**Byzantine-Resilient Decentralized Stochastic Gradient Descent**

—— UBAR （问题在于有强假设   ——需要知道比例，几乎不可能
参考文献：**Byzantine-resilient decentralized stochastic gradient descent**
—— LEARN （交换更新与模型     ——通信过程中剪裁梯度
参考文献：**Collaborative learning in the jungle**
—— SCCLIP （缓和+排他             ——以自我为中心，调整大小
参考文献：**Byzantine-robust decentralized learning via self-centered clipping**

—— Balanced Federal    ——欧氏距离，大小及方向
参考文献：**Byzantine-Robust Decentralized Federated Learning**

方法集合——不完全考虑
![[Pasted image 20251001161554.png]]

### 3、总结与展望
总结1—— 算法分析路径
适用框架——分布、集中
选择的假设
筛选规则、聚合规则、更新规则

总结2—— 结合场景分析（数据同质与异构问题
点出SCCLIP和Balanced Federal的核心区别

展望—— 再说，现在没想法呢。