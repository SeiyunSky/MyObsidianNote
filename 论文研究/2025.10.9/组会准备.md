### 0、什么是联邦学习?
—— 进入中心化方法的引子
    首先是**FedAvg**分析其是什么，应用场景和作用
参考文献：**Communication-Efficient Learning of Deep Networks from Decentralized Data**
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F1%E3%80%81FedAvg)
—— 去中心化的例子
    引出联邦学习的两个大框架
参考文献：**Can Decentralized Algorithms Outperform Centralized Algorithms?A Case Study for Decentralized Parallel Stochastic Gradient Descen**
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F2%E3%80%81Can%20Decentralized%20Algorithms%20Outperform%20Centralized%20Algorithms.A%20Case%20Study%20for%20Decentralized%20Parallel%20Stochastic%20Gradient%20Descen)
![[Pasted image 20251003102027.png|200]]![[Pasted image 20251003102045.png|200]]
___
### 1、讨论联邦学习下拜占庭攻击的概念
——
引出本次汇报核心是分析拜占庭鲁棒性，给出基准目标
什么是拜占庭节点、拜占庭攻击的核心
存在拜占庭节点下的学习目标是什么
给出一个具体的拜占庭攻击应用场景实例
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F3%E3%80%81Byzantine%20Machine%20Learning%20A%20Primer)
![[Pasted image 20251001160544.png]]

——点出防御方法的大类
    两个方向——**排他、缓和**
    [数学逻辑是什么？](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F3.1Byzantine%20Multi-Agent%20Optimization%E2%80%93%20Part%20I%E2%8B%86)
___
——鲁棒性到底指的哪些维度？
**拜占庭弹性**
 - 方向正确性
 - 矩可控性
 ![[Pasted image 20251011165138.png]]
参考文献：**Byzantine Machine Learning: A Primer**
**Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent**
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F3%E3%80%81Byzantine%20Machine%20Learning%20A%20Primer)
___
### 2、内容规划
#### 数学前置知识
**常见的假设**

**常见的聚合更新方式**

#### 中心化
—— 缓和 和 排11他 的两个大类下的方法
几何、坐标中位数 （BGD)  /    Krum
参考文献：
**Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates**——坐标中位数、裁剪均值
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F4.%20%E5%9D%90%E6%A0%87%E4%B8%AD%E4%BD%8D%E6%95%B0%E4%B8%8E%E5%9D%87%E5%80%BC%E8%A3%81%E5%89%AA)

**Distributed Statistical Machine Learning in Adversarial Settings:**  
**Byzantine Gradient Descent**——几何中位数

**Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent**——Krum与Multi-Krum
[点我](obsidian://open?vault=Note&file=%E8%AE%BA%E6%96%87%E7%A0%94%E7%A9%B6%2F2025.10.9%2F5.%20krum%E5%92%8Cmultikrum)


—— Bulyan 和 FLtrust 综合考量两个大方向后的算法分析与利弊
参考文献：
**The Hidden Vulnerability of Distributed Learning in Byzantium**
**Fltrust:Byzantine-robust federated learning via trust bootstrapping**

—— 再来个框架式方法 ASKEL 
参考文献：**AK-SEL: Fast Byzantine SGD**
___
**过渡**：AKSEL 和 FLtrust 一方面违背了分布式下服务器不接触数据的问题、另外一方面，无法用在去中心化方法中
___
#### 去中心化
——Krum/TrimMean的在去中心化下的应用
——为什么需要重新设计、Krum/TrimMean的在去中心化下的问题
参考文献：
**Fault-tolerant multi-agent optimization**
**Byzantine-Resilient Decentralized Stochastic Gradient Descent**

—— UBAR （问题在于有强假设   ——需要知道比例，几乎不可能
参考文献：**Byzantine-resilient decentralized stochastic gradient descent**
—— LEARN （交换更新与模型     ——通信过程中剪裁梯度
参考文献：**Collaborative learning in the jungle**
—— SCCLIP （缓和+排他             ——以自我为中心，调整大小
参考文献：**Byzantine-robust decentralized learning via self-centered clipping**

—— Balanced Federal    ——欧氏距离，大小及方向
参考文献：**Byzantine-Robust Decentralized Federated Learning**
___
### 3、总结与展望
总结1—— 算法分析路径
适用框架——分布、集中
选择的假设
筛选规则、聚合规则、更新规则

总结2—— 结合场景分析（数据同质与异构问题
点出SCCLIP和Balanced Federal的核心区别

展望—— 再说，现在没想法呢。