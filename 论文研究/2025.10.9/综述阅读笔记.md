首先明确问题——联邦学习是一个分布式的【机器学习】框架
↓ 将联邦学习分为集中化与去中心化的原因
![[Pasted image 20250929222357.png]]

——拜占庭联邦学习鲁棒性的开门作 Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for Byzantine robust optimization. In Proceedings of the International Conference on Machine Learning, 2021
- **标准 SGD 的收敛性证明：​**​ 在正常的分布式SGD中，我们假设所有工作节点是诚实的。聚合操作通常是简单的​**​加权平均​**​。平均操作有一个非常好的性质：它不会增加梯度的方差。事实上，如果各个工作节点的梯度估计是独立同分布的，平均操作反而会​**​降低​**​方差。基于这个性质，我们可以顺利地证明算法会收敛到一个平稳点。
- ​**​Byzantine 鲁棒聚合的挑战：​**​ 当引入拜占庭节点后，简单的平均不再安全。我们改用鲁棒聚合器，如几何中位数、均值中位数等。这些聚合器的目标是​**​抵抗异常值​**​。
    - ​**​问题在于：​**​ 这些鲁棒聚合器在过滤掉异常值的同时，也​**​引入了一个偏差​**​。这个偏差意味着，鲁棒聚合后的梯度方向，不再是无偏的全局梯度估计。它可能与真正的梯度方向存在一个固定的夹角。
    - ​**​“方差不匹配”的具体体现：​**​ 在优化理论中，收敛性证明通常需要一个关键条件：​**​随机梯度的方差是有界的​**​。然而，当使用了鲁棒聚合器后，即使每个诚实节点本地的随机梯度方差是有界的，经过鲁棒聚合器处理后的​**​聚合梯度的方差可能会非常大​**​，甚至可能是无界的。这是因为拜占庭节点可以故意发送恶意的梯度值，试图“撬动”鲁棒聚合器（比如几何中位数）的输出，从而导致聚合结果产生巨大波动。

