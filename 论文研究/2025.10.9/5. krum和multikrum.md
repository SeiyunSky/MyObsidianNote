**聚合方式**
![[Pasted image 20251011162134.png]]

**提出背景**：
线性方式的失效
![[Pasted image 20251011162635.png]]

![[Pasted image 20251011162432.png]]

**算法逻辑**
![[Pasted image 20251011162751.png]]
找到每个节点最近的邻居，
邻居间根据彼此之间的距离评分，
得到分数最符合需求的n−f−1个最近邻向量个worker
选择对应分数最符合的n−f−1个最近邻向量个worker进行聚合。
$$O(n^2d)$$
**算法分析**
![[Pasted image 20251011162931.png]]
其中至少有n-f个向量是正确的

对于一个正确的索引i
![[Pasted image 20251011163013.png|100]]
，有局部标准差![[Pasted image 20251011163005.png|200]]

**对于时间复杂度有：**
![[Pasted image 20251011163314.png|600]]

**对于拜占庭弹性有**：
 **诚实梯度标准假设 (IID, 无偏, 有界方差）**——诚实行为的核心
这里有两个大前提 
- **$2f + 2 < n$**——恶意节点上限
- $η(n, f) ⋅ √d ⋅ σ < ||g||$**——信噪比关系
![[Pasted image 20251011172200.png|600]]
![[Pasted image 20251011164832.png|600]]
![[Pasted image 20251011165204.png|600]]
定义：
![[Pasted image 20251011164908.png|600]]$\eta(n, f) \overset{\text{def}}{=} \sqrt{\frac{2(n-f+f\cdot(n-f-2)+f^2\cdot(n-f-1))}{n-2f-2}}$

首先——**推导诚实向量分数的最大期望**：
![[Pasted image 20251011165307.png|300]]
**利用诚实向量和真实向量方差的有界性**
![[Pasted image 20251011165508.png|600]]
对于这种初步上界，考虑到拜占庭节点并非简单远离，而是会采用更优的策略——靠近诚实节点以冒充其，命题中的**η(n,f)函数**的分子来源于更紧的上界。

![[Pasted image 20251011165720.png]]

**下一步是求误差下界**
![[Pasted image 20251011165809.png|400]]
对于任意一个诚实邻居，使用范数基本的数学方法
![[Pasted image 20251011165852.png|300]]
得到最终输出到g的距离可以被其到诚实邻居与诚实邻居到g的距离约束。而对于所有的诚实邻居，累加后可以得到
$$\sum_{k \in C'} \|V_{i^*} - g\|^2 = |C'| \cdot \|V_{i^*} - g\|^2$$
$$\sum_{k \in C'} \left( 2\|V_{i^*} - V_k\|^2 + 2\|V_k - g\|^2 \right) = 2 \sum_{k \in C'} \|V_{i^*} - V_k\|^2 + 2 \sum_{k \in C'} \|V_k - g\|^2$$
其中，$2 \sum_{k \in C'} \|V_{i^*} - V_k\|^2$只是$score(i*)$的子集，因此有
![[Pasted image 20251011170751.png|400]]
![[Pasted image 20251011170829.png|300]]

**对其取期望，得到多次krum的上界**，因为C‘是随机变量，代表了诚实节点的数量，因此取n-2f-2这个下界

使用krum分数小于等于score(j)（被选中的小于等于其他任意的点），可以得到![[Pasted image 20251011171226.png|500]]
**根据论文内的数学方法**可以看到最后一项能被第一项吸收（**因为当前对攻击者的假设过于乐观**），利用原来的E\[score(j)]的上界，可以得到
![[Pasted image 20251011171608.png|500]]


最终根据**詹森不等式**$E[X^2]≥(E[X])^2$
有
![[Pasted image 20251011171721.png|200]]
说明——krum输出的期望向量，

必然落在以g为球心，半径为r的高维球体内部
也就是说，在最坏的情况下
$\sin(\text{角}) = \frac{\text{对边}}{\text{斜边}}$其中边长分别为斜边∥g∥、∥E\[KR]∥、和 对边∥E\[KR]−g∥
![[Pasted image 20251011171813.png]]
$$\sin \alpha \overset{\text{def}}{=} \frac{r}{|g|} = \frac{\eta(n, f) \cdot \sqrt{d} \cdot \sigma}{|g|}$$


**对于SGD收敛性**
当前已知，krum输出的期望向量必然落在以真实梯度r为半径的高维球体内部
![[Pasted image 20251012200542.png|300]]
利用向量恒等式$<a,b> = \frac{1}{2}(||a||^2+||b||^2-||a-b||^2)$
得到内积的下界
$$\left\langle \nabla F\left(w_{t}\right), \mathbb{E}\left[g_{\text{krum}}\right] \right\rangle \geq \frac{1}{2}\left( \left\|\nabla F\left(w_{t}\right)\right\|^{2} + \left\|\mathbb{E}\left[g_{\text{krum}}\right]\right\|^{2} - r^{2} \right)$$
为了保证内积为正，右边的下界需要大于0

**利用柯西-施瓦茨不等式**
$$\langle \nabla F(w_t), \mathbb{E}[g_{krum}] - \nabla F(w_t) \rangle \ge -|\nabla F(w_t)| \cdot |\mathbb{E}[g_{krum}] - \nabla F(w_t)|$$
重新带入到刚刚的下界中
$$\langle \nabla F(w_t), \mathbb{E}[g_{krum}] \rangle \ge |\nabla F(w_t)|^2 - |\nabla F(w_t)| \cdot r$$
$$\langle \nabla F(w_t), \mathbb{E}[g_{krum}] \rangle \ge |\nabla F(w_t)| \cdot (|\nabla F(w_t)| - r)$$
要让内积大于0，只需要有前提**理想损失函数梯度优化方向大于噪声影响**

**利用L-平滑性，将模型更新规则$w_{t+1}−w_t=−η⋅g_{krum}$带入**
$$\mathbb{E}\left[F\left(w_{t+1}\right)\right] \leq F\left(w_{t}\right)-\eta \mathbb{E}\left[\left\langle\nabla F\left(w_{t}\right), g_{\text{krum}}\right\rangle\right] +\frac{L\eta^{2}}{2} \mathbb{E}\left[\left\|g_{\text{krum}}\right\|^{2}\right]$$
**接下来需要证明L光滑性导致的误差有界**
首先  $score(g_{krum}) \le score(V_j)$
而诚实梯度的期望是有界的
![[Pasted image 20251012202529.png|300]]
将其转化到n-f-1个最近邻的距离和上
![[Pasted image 20251012202602.png|300]]
带入刚刚第一个不等式，并且取期望，有
![[Pasted image 20251012202711.png|300]]
**看这个式子的右边两项是否有上界**
对于诚实梯度分数：$\mathbb{E}[score(V_j)] \le \mathbb{E}\left[\sum_{k \in C, k \ne j} |V_j-V_k|^2\right] = (n-f-1) \cdot \mathbb{E}[|V_j-V_k|^2]$
对于任何一个诚实梯度有：$\mathbb{E}[|V_k|^2] = \mathbb{E}[|(V_k-g)+g|^2] = \mathbb{E}[|V_k-g|^2 + |g|^2 + 2\langle V_k-g, g \rangle]= \mathbb{E}[|V_k-g|^2] + |g|^2 + 2\langle \mathbb{E}[V_k-g], g \rangle = d\sigma^2 + |g|^2$
最终就能得到，也就是SGD算法的有界性，通过学习率降低，保证收益的持续性：
$$\mathbb{E}\left[\sum_{k \in C'} |V_k|^2\right] = \mathbb{E}[|C'| \cdot \mathbb{E}[|V_k|^2]] = \mathbb{E}[|C'|] (d\sigma^2 + |g|^2)$$



