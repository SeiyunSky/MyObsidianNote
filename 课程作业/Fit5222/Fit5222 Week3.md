![[Pasted image 20250714105745.png]]
![[Pasted image 20250714102803.png]]
  In the A* algorithm, using a binary heap as the priority queue yields a time complexity of $O(b^d log N)$, where b is the branching factor, d is the solution depth, and N is the maximum number of nodes in the open set. This logarithmic complexity stems from the heap's efficient `O(log N)` insert and extract-min operations. When switching to an unsorted array implementation, while insertion reduces to `O(1)`, extracting the minimum element requires an O(N) linear scan, degrading the overall complexity to $O(b^d × N)$ with $O(N²)$ worst-case performance. This polynomial growth restricts array usage to trivial problems (N<10), whereas binary heaps (or more generally k-ary heaps) maintain logarithmic efficiency, making them the standard choice for A* implementations.
  The fundamental difference lies in priority queue operation costs: heaps preserve partial ordering through tree structures to guarantee logarithmic-time operations, while arrays' linear organization lacks inherent ordering, forcing full scans for each minimum extraction. This disparity grows exponentially with search space size—for N=1,000, array extraction becomes ≈100× slower than heaps. Thus, despite k-ary heaps requiring additional structural overhead, their asymptotic complexity advantage makes them strictly superior for graph search scenarios.

![[Pasted image 20250714102815.png]]
![[Pasted image 20250714102829.png]]


![[Pasted image 20250714103346.png]]
a、In the analysis of adjacency graphs and portal graphs for navigation meshes, both data structures meet the requirement of completeness. The adjacency graph ensures the existence of paths by establishing a fully connected topological relationship between polygonal cells, while the portal graph guarantees the existence of solutions by explicitly modeling the connectivity of all traversable passages. Specifically, if and only if a physically feasible path exists in the navigation mesh, the adjacency graph can provide a corresponding feasible solution at the abstract level through the connectivity of geometric center points, and the portal graph can do so through the visibility between portal nodes. 
This completeness stems from their comprehensive spatial coverage of the original navigation mesh and their ability as approximately optimal abstraction techniques to retain the entire solution space. In practical applications, the completeness of the adjacency graph is reflected in its strict preservation of the adjacency relationship between polygons, while the completeness of the portal graph is achieved through sufficient sampling of the free space by portal points. Both meet the requirement of ensuring the existence of solutions for search algorithms through different abstraction methods.

b、In navigation mesh path planning, portal graphs typically offer higher-quality solutions. The advantages of portal graphs are reflected in their ability to generate near-optimal paths in scenarios involving narrow passages, while adjacency graphs perform better when directly connecting the center points of polygons. However, portal graphs may produce redundant detours when portal points are unevenly distributed, and adjacency graphs tend to form zigzag polygonal lines that significantly deviate from the actual shortest path in the presence of complex obstacles. 
The quality difference between these two methods essentially stems from their abstraction approaches: portal graphs achieve paths that are more consistent with real-world movements by accurately modeling physically feasible passages, whereas adjacency graphs rely on a simplified representation of polygon topological relationships, resulting in higher computational efficiency but lower path precision.
Improvement schemes need to balance computational overhead and path quality. For example, increasing the density of portal points can improve path quality but will significantly increase the complexity of the graph structure.

c、The core methods to improve the quality of portal graph solutions include increasing portal point density, introducing dynamic weighting mechanisms, and constructing hierarchical abstract structures. All these optimizations require a trade-off between computational efficiency and path quality.
In state grids, methods to ensure the feasibility of actions between non-adjacent tiles include Dubins path calculation, three-layer collision detection mechanisms, connectivity constraints, and probabilistic verification. The core lies in covering kinematic constraints, real-time environmental changes, and execution error tolerances through a multi-dimensional verification system.

![[Pasted image 20250714104638.png]]
To ensure the feasibility of actions between non-adjacent tiles in a state grid, it is necessary to pre-validate the physical realizability of the actions, design heuristic functions to evaluate state transition costs, and employ path-smoothing algorithms to eliminate discontinuous jumps, while ensuring consistency between the system's dynamic model and action logic, and introducing intermediate states as transitions when necessary.

![[Pasted image 20250714104604.png]]
In pathfinding, although the differential heuristic is generally more informed, it is not absolutely dominant. For example, in a simple 8-connected grid scenario, suppose we want to find a path from (0,0) to (3,3). If the differential heuristic has an incorrect heuristic function setup , its estimated cost may be too high. However, the octile distance (suitable for 8-connected movement, calculated to be approximately 4.242, close to the actual optimal path cost) and the straight-line distance (calculated to be approximately 4.242, also close to the actual value) can estimate more accurately. In such cases, they provide better estimates, showing that when the differential heuristic is improperly set up, the octile or straight-line distance can be superior.

![[Pasted image 20250714105507.png]]
To automatically select pivot points in the A* algorithm, a method combining heuristic functions and region partitioning can be employed. First, the state space is decomposed into multiple regions via spatial grid division. The heuristic estimates of each region's center point are then computed, and the top-k region centers with the best heuristic values are selected as pivot points. During the search process, pivot selection is dynamically updated , while the principles of Jump Point Search are introduced to prioritize expanding nodes near the pivot points. This approach significantly reduces node expansions while ensuring optimality.

![[Pasted image 20250714105617.png]]
The analysis is as follows: 
The construction phase requires O(n²) preprocessing time (where n is the number of states) to compute all-pairs shortest paths and compress the storage. 
The query phase has a time complexity of O(1) (retrieving the initial move via a single table lookup). 
Extracting the complete path requires O(L) time (where L is the path length)
The space complexity is O(n).

![[Pasted image 20250714105753.png]]

![[Pasted image 20250714110117.png]]
![[Pasted image 20250714110126.png]]
![[Pasted image 20250714113806.png]]
The Differential Heuristic outperforms Octile Distance in obstacle - rich 8 - connected grids. DH leverages precomputed pivot distances to account for real - world path deviations caused by obstacles, reducing unnecessary node expansions. For example, in our test map with a circular barrier, DH guided the search toward the mandatory detour path earlier, pruning the search tree more aggressively. Octile Distance, a geometric estimate, failed to account for obstacles, leading to more node expansions as it clung to shorter (but blocked) paths. While DH requires upfront pivot - table precomputation, it reuses these tables for dynamic start locations, saving time compared to recalculating Octile estimates repeatedly.
Adding pivots initially improves DH accuracy by increasing map coverage, as seen when adding (1,1), (8,8), etc., which better reflected the detour path,DH rose to 14.00, aligning with the actual route. However, returns diminish when the marginal accuracy gain is outweighed by precomputation time and memory costs. For small maps, adding pivots beyond a “critical mass”provides no new structural insights but consumes resources, making further additions inefficient.A perfect - distance heuristic  is theoretically more accurate, as it reflects the true shortest path. However, it is impractical for dynamic start locations due to storage and precomputation overhead. DH balances accuracy and flexibility: it reuses pivot tables for any start location but introduces error. For our test case, DH error suggests room for improvement, but averaged across all starts, DH remains efficient. For fixed goals with changing starts, DH outperforms UCS - based heuristics in resource usage, while UCS excels in accuracy for static start - goal pairs.